[Dims]
latent_dim = 8
bottleneck_dim = 16
hidden_dims = 32, 16

[Hyperparameters]
hyperopt = 0
n_trials = 100

num_epochs = 100
dropout = 0.2
batch_size = 256

# MAE
mask_ratio = 0.7
num_heads = 2

# VAE
beta_min = 0.1

# learning rate
initial_lr = 0.0001
max_lr = 0.001
final_lr = 0.00001
# scheduler = StepLR, CosineAnnealingLR, ReduceLROnPlateau, ExponentialLR or WarmupCosine
scheduler = ReduceLROnPlateau
step_size = 10
gamma = 0.85
patience = 5

# optimizer = SGD, Adam, AdamW, Adagrad, RMSprop
optimizer = Adam


[Model]
training_model = VAE
save_model = 1
test_mode = 1
model_path = Models/VAE_1000_01_21_19_31.pth

[Data]
num_workers = 8
file_path = data2/allando_v_savvaltas_alacsony_v5_combined.csv
data_dir = data2
num_manoeuvres = 2
train_size = 0.8
val_size = 0.1
seed = 69

[Callbacks]
plot = 0
neptune_project = mitrengamark/Autoencoder-Identification
neptune_token = "eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI4NWJiMzAyNC02MWE2LTRhZDItYTgxNi0wZjg2ZjFjNTg5NTEifQ=="
tolerance = 0.001