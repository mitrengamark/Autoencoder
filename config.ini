[Dims]
latent_dim = 4
hidden_dim_0 = 16
hidden_dim_1 = 32

[Hyperparameters]
num_epochs = 10
dropout = 0.1
batch_size = 128

# MAE
mask_ratio = 0.75

# VAE
beta = 1

# learning rate
max_lr = 0.1
lr = 0.001
# scheduler = StepLR, CosineAnnealingLR, ReduceLROnPlateau, ExponentialLR or WarmupCosine
scheduler = WarmupCosine
step_size = 10
gamma = 0.5
patience = 5
warmup_epochs = 3

[Model]
training_model = MAE
save_model = 0
test_mode = 1

[Data]
file_path = data2/allando_v_savvaltas_alacsony_v5_combined.csv
train_size = 0.8
val_size = 0.1
seed = 69

[Callbacks]
plot = 0
neptune_project = mitrengamark/Autoencoder-Identification
neptune_token = "eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI4NWJiMzAyNC02MWE2LTRhZDItYTgxNi0wZjg2ZjFjNTg5NTEifQ=="